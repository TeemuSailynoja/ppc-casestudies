[
  {
    "objectID": "case_studies/ordinal/ordinal.html",
    "href": "case_studies/ordinal/ordinal.html",
    "title": "Calibrarion of Ordinal Posterior Predictions",
    "section": "",
    "text": "Imports & options\nlibrary(\"bayesplot\")\nlibrary(\"cmdstanr\")\nlibrary(\"ggplot2\")\nlibrary(\"khroma\")\nlibrary(\"quartoExtra\")\n\n\n# Source for the modified reliability plot\nsource(\"../../code/helpers.R\")\n\ngood_theme <- bayesplot::theme_default(base_family = \"Sans\") + theme(\n  axis.text = element_text(colour = \"#666666\", size = 12),\n  axis.ticks = element_line(colour = \"#666666\"),\n  title = element_text(colour = \"#666666\", size = 16),\n  plot.subtitle = element_text(colour = \"#666666\", size = 14),\n  legend.text = element_text(colour = \"#666666\", size = 12),\n  legend.title = element_text(colour = \"#666666\", size = 14),\n  axis.line = element_line(colour = \"#666666\"))\n\ntheme_set(good_theme)\nbayesplot_theme_set(good_theme)\ncolor_scheme_set(scheme = c(unname(colour(\"vibrant\")(7)[c(3,2,5,4,1,6)])))\n\nscale_colour_discrete = scale_colour_vibrant\nscale_fill_discrete = scale_fill_vibrant\n\n\n# darkmode_theme_set(\n#     dark = ggthemes::theme_stata(scheme = \"s1rcolor\"),\n#     light = ggthemes::theme_stata(scheme = \"s1color\")\n# )\n\n\nSEED <- 236543\nset.seed(SEED)\nSAVE_FITS = TRUE\nThis notebook highlights posterior predictive visualizations when the posterior predictive distribution is ordinal.\nAs shown below, the ordinal nature of the predictions allows us to use the cumulative posterior predictive mass function to assess the calibration of the posterior."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Case Studies in Visual Posterior Predictive Checks",
    "section": "",
    "text": "Pitfalls of density plots\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nTeemu Säilynoja\n\n\n\n\n\n\n  \n\n\n\n\nContinuous Predictive Distribution\n\n\nShould I use a KDE?\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nTeemu Säilynoja\n\n\n\n\n\n\n  \n\n\n\n\nCalibrarion of Ordinal Posterior Predictions\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nTeemu Säilynoja\n\n\n\n\n\n\n  \n\n\n\n\nPPC: Ordinal predictions\n\n\nBayesian network model for predicting the number of pregnancies in an IVF treatment.\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nTeemu Säilynoja\n\n\n\n\n\n\n  \n\n\n\n\nPPC Visualizations for Categorical Data\n\n\nPalmer Penguins\n\n\nCalibration plots for the easy tasks of identifying penguin species in the Palmer Penguins data set.\n\n\n\n\n\n\nJan 30, 2023\n\n\nTeemu Säilynoja\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "case_studies/categorical/categorical_palmer_penguins.html",
    "href": "case_studies/categorical/categorical_palmer_penguins.html",
    "title": "PPC Visualizations for Categorical Data",
    "section": "",
    "text": "Code\nlibrary(\"bayesplot\")\nlibrary(\"cmdstanr\")\nlibrary(\"ggplot2\")\nlibrary(\"khroma\")\nlibrary(\"quartoExtra\")\n\n\n# Source for the modified reliability plot\nsource(\"../../code/helpers.R\")\n\ngood_theme <- bayesplot::theme_default(base_family = \"Sans\") + theme(\n  axis.text = element_text(colour = \"#666666\", size = 12),\n  axis.ticks = element_line(colour = \"#666666\"),\n  title = element_text(colour = \"#666666\", size = 16),\n  plot.subtitle = element_text(colour = \"#666666\", size = 14),\n  legend.text = element_text(colour = \"#666666\", size = 12),\n  legend.title = element_text(colour = \"#666666\", size = 14),\n  axis.line = element_line(colour = \"#666666\"))\n\ntheme_set(good_theme)\nbayesplot_theme_set(good_theme)\ncolor_scheme_set(scheme = c(unname(colour(\"vibrant\")(7)[c(3,2,5,4,1,6)])))\n\nscale_colour_discrete = scale_colour_vibrant\nscale_fill_discrete = scale_fill_vibrant\n\nsource(\"../../code/helpers.R\")\n\nSAVE_MODEL = TRUE\n\n\nCalibration plots for the easy tasks of identifying penguin species in the Palmer Penguins data set.\n\nThe data\n\n\nCode\nif (FALSE) {\n  data(\"iris\")\n  X <- dplyr::select(na.omit(iris), -c(\"Species\"))\n  y <- as.numeric(iris$Species)\n} else {\n  library(palmerpenguins)\n  data(\"penguins\")\n  X <- na.omit(penguins)[, c(3,4,5,6)]\n  y <- as.factor(na.omit(penguins)$species)\n}\n\n\n\n\nCode\nggplot(X, aes(x = bill_length_mm, y = bill_depth_mm, colour = y)) +\n  geom_point() +\n  xlab(\"Bill length (mm)\") +\n  ylab(\"Bill depth (mm)\") +\n  labs(colour = \"Species\") +\n  legend_move(position = \"top\")\n\n\n\n\n\n\n\nThe model\n\n\nCode\nif (FALSE) {\n  # model directory contains the required model\n  # load precompiled model\n} else {\n  model_code  = \"\n  data {\n    int N; // number of observations\n    int D; // number of features\n    int N_classes; // number of classes\n    matrix [N, D] X; // observation data\n    array[N] int <lower = 1, upper = N_classes> y; // target values {1,..., N_classes}\n  }\n  \n  transformed data {\n    matrix[D + 1, N] X_stn;\n    X_stn[D + 1, ] = rep_row_vector(1, N);\n    for (d in 1:D) {\n      X_stn[d,] = to_row_vector((X[, d] - mean(X[, d])) / sd(X[, d]));\n    }\n  }\n  \n  parameters {\n    matrix[N_classes, D + 1] W;\n  }\n  \n  transformed parameters {\n    matrix[N_classes, N] Beta;\n    for (c in 1:N_classes) {\n      Beta[c, ] =  W[c, ] * X_stn;\n    }\n  }\n  \n  model {\n    for (d in 1:(D + 1)) {\n      for (c in 1:N_classes) {\n        target += normal_lpdf(W[c, d] | 0, 1);\n      }\n    }\n    for (n in 1:N) {\n      target += categorical_logit_lpmf(y[n] | Beta[,n]);\n    }\n  }\n  \n  generated quantities {\n    vector[N] yrep;\n    for (n in 1:N) {\n      yrep[n] = categorical_logit_rng(Beta[,n]);\n    }\n    matrix[N,N_classes] lpd;\n    for (n in 1:N) {\n      for (c in 1:N_classes) {\n        lpd[n, c] = categorical_logit_lpmf(c | Beta[,n]);\n      }\n    }\n  }\n  \"\n  model = cmdstan_model(write_stan_file(\n      model_code,\n      dir = if(SAVE_MODEL) \"../../code/stan-models\" else tempdir(),\n      basename = \"penguins_glm\",\n    ))\n}\n\n\n\n\nCode\nfit <- model$sample(\n  data = list(N = nrow(X),\n              D = ncol(X),\n              N_classes = length(unique(y)),\n              X = X,\n              y = as.numeric(y)),\n  parallel_chains = 4,\n  refresh = 0)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 5.9 seconds.\nChain 4 finished in 5.9 seconds.\nChain 3 finished in 6.2 seconds.\nChain 2 finished in 6.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 6.1 seconds.\nTotal execution time: 6.5 seconds.\n\n\n\n\nThe calibration\nThe common approach of plotting a bar char of the observations overlaid with posterior means and 95% confidence intervals only gives a crude idea of the calibration of the model predictions.\n\n\nCode\nppc_bars(as.numeric(y), fit$draws(variables = \"yrep\", format = \"matrix\")) +\n  scale_x_continuous(breaks = 1:3, labels = levels(y))\n\n\n\n\n\n\n\nCode\nplot_dotted_reliabilitydiag(x = exp(colMeans(fit$draws(variables = paste(paste(\"lpd[\", 1:nrow(X), sep=\"\"), \",1]\", sep=\"\"), format = \"matrix\"))), y = as.numeric(y == levels(y)[1]), quantiles = 20) + labs(title = paste(\"Calibration:\", levels(y)[1], \"vs. Others\"))\n\n\n\n\n\n\n\nCode\nplot_dotted_reliabilitydiag(x = exp(colMeans(fit$draws(variables = paste(paste(\"lpd[\", 1:nrow(X), sep=\"\"), \",2]\", sep=\"\"), format = \"matrix\"))), y = as.numeric(y == levels(y)[2]), quantiles = 20) + labs(title = paste(\"Calibration:\", levels(y)[2], \"vs. Others\"))\n\n\n\n\n\n\n\nCode\nplot_dotted_reliabilitydiag(x = exp(colMeans(fit$draws(variables = paste(paste(\"lpd[\", 1:nrow(X), sep=\"\"), \",3]\", sep=\"\"), format = \"matrix\"))), y = as.numeric(y == levels(y)[3]), quantiles = 20)  + labs(title = paste(\"Calibration:\", levels(y)[3], \"vs. Others\"))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "case_studies/ordinal/ordinal.html#data-set",
    "href": "case_studies/ordinal/ordinal.html#data-set",
    "title": "Calibrarion of Ordinal Posterior Predictions",
    "section": "Data set",
    "text": "Data set\nBelow, I use synthetic data generated from observations from \\(K\\) Gaussians with varying means.\n\\[\\begin{align}\nx_n &\\sim \\mathcal N\\!\\!\\left(k, 0.5^2\\right), &\\text{for } n \\in\\{1,\\dots,N\\}\\\\\nk &\\sim \\text{Categorical}(\\theta_k),&\\\\\n\\theta_k &= \\frac 1 K, &\\text{for } k \\in \\{1, \\dots, K\\}.\n\\end{align}\\]\n\n\nData generation\nK <- 5\nN <- 1500\nsigma <- .5\nc <- sample(1:K, N, replace = T)\nx <- rnorm(N, c, sigma)\nstandata_gmm <- list(K = K,\n                     N = N,\n                     x = x,\n                     y = c,\n                     sigma = sigma)\n\n\n\n\nCode\nggplot(data.frame(standata_gmm)) +\n  geom_density(aes(x = x,\n                   colour = as.factor(y),\n                   fill = as.factor(y),\n                   group = as.factor(y)), alpha = .5) +\n  legend_none() +\n  xlab(\"\") + ylab(\"\") + ggtitle(paste(\"The data\", sep = \"\"))"
  },
  {
    "objectID": "case_studies/ordinal/ordinal.html#model",
    "href": "case_studies/ordinal/ordinal.html#model",
    "title": "Calibrarion of Ordinal Posterior Predictions",
    "section": "Model",
    "text": "Model\nThe data is fit with two models, both structured to first normalize the data and then fit a GMM with K = 5`\n\n\nRead model code\ngmm <- cmdstan_model(\"../../code/stan-models/gmm_classifier.stan\")\ngmm\n\n\ndata {\n  int<lower=2> K;                   // Number of classes\n  int<lower=0> N;                   // Total number of observations\n  array[N] int<lower=1, upper=K> y; // Target classes\n  vector[N] x;                      // Observed predictor values\n  real<lower=0> sigma;              // User supplied standard deviation\n  int correct_sigma;                // How to handle sigma, see below.\n}\n\ntransformed data{\n  vector[N] x_st;\n  real Sigma;\n\n  // Standardize data\n  x_st = (x - mean(x)) / sd(x);\n\n  // Maybe remember to scale sigma accordingly\n  if (correct_sigma == 1) {\n    Sigma = sigma / sd(x);\n  } else {\n    Sigma = sigma;\n  }\n}\n\nparameters {\n  // Inferred means.\n  ordered[K] c;\n  simplex[K] p_c;\n}\n\nmodel {\n  // Prior\n  c ~ normal(0,1);\n  p_c ~ dirichlet(rep_vector(1,K));\n\n  // Likelihood\n  for (n in 1:N) {\n    target += normal_lpdf(x_st[n] | c[y[n]], Sigma);\n  }\n}\n\ngenerated quantities {\n  // Posterior predictive sample\n  vector[N] yrep;\n  // For each observation, posterior predictive mass of classes\n  array[N] vector[K] ppm;\n\n  for (n in 1:N) {\n    for (k in 1:K) {\n      ppm[n, k] = normal_lpdf(x_st[n] | c[k], Sigma);\n    }\n    ppm[n, ] = softmax(ppm[n, ]);\n    yrep[n] = categorical_rng(ppm[n, ]);\n  }\n}\n\n\n\n\nrun CmdStanR\nfit_1 <- tryCatch(\n  expr = {readRDS(paste(\"../../code/stan-models/fits/gmm_classifier_1_\",SEED,\".RDS\", sep=\"\"))},\n  error = function(e) {\n    fit <- gmm$sample(data = c(standata_gmm, list(correct_sigma = 0)),\n                      parallel_chains = 4,\n                      refresh = 0,\n                      seed = SEED,\n                      show_messages = F)\n    if (SAVE_FITS) {fit$save_object(\n      paste(\"../../code/stan-models/fits/gmm_classifier_1_\",SEED,\".RDS\", sep=\"\"))}\n    return(fit)},\n  finally = {message(\"Finished model 1.\")})\n\nfit_2 <- tryCatch(\n  expr = readRDS(paste(\"../../code/stan-models/fits/gmm_classifier_2_\",SEED,\".RDS\", sep=\"\")),\n  error = function(e) {\n    fit <- gmm$sample(data = c(standata_gmm, list(correct_sigma = 1)),\n               parallel_chains = 4,\n               refresh = 0,\n               seed = SEED,\n               show_messages = F)\n    if (SAVE_FITS) {fit$save_object(\n      paste(\"../../code/stan-models/fits/gmm_classifier_2_\",SEED,\".RDS\", sep=\"\"))}\n    return(fit)},\n  finally = {message(\"Finished model 2.\")})\n\n\n\n\nCode\np_1 <- matrix(colMeans(fit_1$draws(variables = \"ppm\", format = \"matrix\")), ncol = K)\np_2 <- matrix(colMeans(fit_2$draws(variables = \"ppm\", format = \"matrix\")), ncol = K)"
  },
  {
    "objectID": "case_studies/ordinal/ordinal.html#ppc",
    "href": "case_studies/ordinal/ordinal.html#ppc",
    "title": "Calibrarion of Ordinal Posterior Predictions",
    "section": "PPC",
    "text": "PPC\n\n\n\n\nCode\nppc_bars(y = as.numeric(c),\n         yrep = fit_1$draws(variables = \"yrep\",format = \"matrix\")) +\n  ggtitle(\"Model 1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nppc_bars(y = as.numeric(c),\n         yrep = fit_2$draws(variables = \"yrep\", format = \"matrix\")) +\n  ggtitle(\"Model 2\") +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor (k in 1:(K - 1)) {\n  p1 <- plot_dotted_reliabilitydiag(\n    y = as.numeric(c <= k),\n    x = if (k != 1) pmin(1, rowSums(p_1[, 1:k])) else p_1[, k],\n    quantiles = K * N / 100,\n    dot_scale = .5) +\n    ggtitle(paste(\"Model 1: P(y <= \", k, \")\", sep=\"\"),\n            subtitle = \"1 dot = 100 observations\")\n  \n  print(p1)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor (k in 1:(K - 1)) {\n  p2 <- plot_dotted_reliabilitydiag(\n    y = as.numeric(c <= k),\n    x = if (k != 1) pmin(1, rowSums(p_2[, 1:k])) else p_2[, k],\n    quantiles = K * N / 100,\n    dot_scale = .5) +\n    ggtitle(paste(\"Model 2: P(y <= \", k, \")\", sep=\"\"),\n            subtitle = \"1 dot = 100 observations\")\n  \n  print(p2)\n  }"
  },
  {
    "objectID": "case_studies/ordinal/bn_classifier.html",
    "href": "case_studies/ordinal/bn_classifier.html",
    "title": "PPC: Ordinal predictions",
    "section": "",
    "text": "Code\nlibrary(cmdstanr)\nlibrary(DirichletReg)\nlibrary(bayesplot)\nlibrary(ggplot2)\nlibrary(reliabilitydiag)\nlibrary(khroma)\n\n# Source for the modified reliability plot\nsource(\"../../code/helpers.R\")\n\ngood_theme <- bayesplot::theme_default(base_family = \"Sans\") + theme(\n  axis.text = element_text(colour = \"#666666\", size = 12),\n  axis.ticks = element_line(colour = \"#666666\"),\n  title = element_text(colour = \"#666666\", size = 16),\n  plot.subtitle = element_text(colour = \"#666666\", size = 14),\n  legend.text = element_text(colour = \"#666666\", size = 12),\n  legend.title = element_text(colour = \"#666666\", size = 14),\n  axis.line = element_line(colour = \"#666666\"))\n\ntheme_set(good_theme)\nbayesplot_theme_set(good_theme)\ncolor_scheme_set(scheme = c(unname(colour(\"vibrant\")(7)[c(3,2,5,4,1,6)])))\n\nscale_colour_discrete = scale_colour_vibrant\nscale_fill_discrete = scale_fill_vibrant\n\nSEED <- 236543\nset.seed(SEED)\nSAVE_FITS = TRUE\nSIM_IIRM = TRUE # whether to use the inferred parameter values from the paper."
  },
  {
    "objectID": "case_studies/ordinal/bn_classifier.html#parameters-of-interest",
    "href": "case_studies/ordinal/bn_classifier.html#parameters-of-interest",
    "title": "PPC: Ordinal predictions",
    "section": "Parameters of interest",
    "text": "Parameters of interest\n\n\nCode\ncolor_scheme_set(scheme = color(\"BuRd\")(13)[6:1])\nmcmc_areas(fit$draws(variables = \"pU\", format = \"matrix\")) + vline_at(v = pU, colour = \"#666666\")\n\n\n\n\n\n\n\nCode\nmcmc_areas(fit$draws(variables = \"pE\", format = \"matrix\")[,-1]) + vline_at(v = pE[-1], colour = \"#666666\")"
  },
  {
    "objectID": "case_studies/ordinal/bn_classifier.html#population-parameters",
    "href": "case_studies/ordinal/bn_classifier.html#population-parameters",
    "title": "PPC: Ordinal predictions",
    "section": "Population parameters",
    "text": "Population parameters\nFor some reason, the authors also model the frequencies of the different observation categories in the population. These are recovered quite well.\n\n\nCode\nmcmc_areas(fit$draws(variables = \"pA\", format = \"matrix\")) + vline_at(v = pA, colour = \"#666666\")\n\n\n\n\n\n\n\nCode\nmcmc_areas(fit$draws(variables = \"pS\", format = \"matrix\")) + vline_at(v = pS, colour = \"#666666\")"
  },
  {
    "objectID": "case_studies/continuous/kde_with_discontinuities.html",
    "href": "case_studies/continuous/kde_with_discontinuities.html",
    "title": "Continuous Predictive Distribution",
    "section": "",
    "text": "Imports & options\nlibrary(\"bayesplot\")\nlibrary(\"cmdstanr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtrace\")\nlibrary(\"khroma\")\n\n\n# Source for the modified reliability plot\nsource(\"../../code/helpers.R\")\n\ngood_theme <- bayesplot::theme_default(base_family = \"Sans\") + theme(\n  axis.text = element_text(colour = \"#666666\", size = 12),\n  axis.ticks = element_line(colour = \"#666666\"),\n  title = element_text(colour = \"#666666\", size = 16),\n  plot.subtitle = element_text(colour = \"#666666\", size = 14),\n  legend.text = element_text(colour = \"#666666\", size = 12),\n  legend.title = element_text(colour = \"#666666\", size = 14),\n  axis.line = element_line(colour = \"#666666\"))\n\ntheme_set(good_theme)\nbayesplot_theme_set(good_theme)\ncolor_scheme_set(scheme = c(unname(colour(\"vibrant\")(7)[c(3,2,5,4,6,1)])))\n\nscale_colour_discrete = scale_colour_vibrant\nscale_fill_discrete = scale_fill_vibrant\n\nSEED <- 236543\nset.seed(SEED)\n\n\n\n\nData generation\nn_rep <- 10\nN <- 250\nfreq_na <- 0.1\nx <- rnorm(N, 1, .5)\nx[rbinom(N, 1, freq_na) == 1] <- 0\nmu_0 <- 0\nnu <- 1\nalpha <- 1\nbeta <- 1\n\nalpha_post <- alpha + N / 2\nbeta_post <- beta + 0.5 * (N - 1) * var(x) +\n  (N * nu * (mean(x) - mu_0) ** 2) / (2 * (nu + N))\n\nsd_post <- sqrt(1 / rgamma(n_rep, alpha_post, beta_post))\nnu_post <- nu + N\nmu_post <- rnorm(n_rep, nu * mu_0 + sum(x) / (nu * N), sd = sd_post / sqrt(nu_post) )\n\nx_post <- c(replicate(N, rnorm(n_rep, mu_post, sd_post)))\nrep_id <- rep(c(1:n_rep), each = N)\n\n\n\n\nCode\np_dens <- ppc_dens_overlay(x, matrix(x_post, nrow = n_rep))\np_dots <- ggplot() +\n  stat_dots(aes(x = x),\n            quantiles = 100,\n            fill = \"transparent\",\n            colour = layer_data(p_dens, 2L)$colour[1]) +\n  stat_density(aes(x = x),\n               geom = \"line\",\n               colour = layer_data(p_dens, 2L)$colour[1]) +\n  xlab(\"\")\np_hist <- ggplot() +\n  geom_histogram(aes(x = x),\n                 colour = layer_data(p_dens, 2L)$colour[1],\n                 fill = layer_data(p_dens, 2L)$colour[1],\n                 alpha = .2)\n\n\n\n\nCode\npit <- function(x, fig_data, idx) {\n  kde <- fig_data[fig_data$group == idx, c(\"x\",\"y\")]\n  Kde <- list(x = kde$x, y = (kde$x[2] - kde$x[1]) * cumsum(kde$y))\n  sapply(x, function(x_i) Kde$y[which.max(Kde$x >= x_i)])\n}\n\n\n\n\nCode\np_ecdf <- ppc_pit_ecdf(pit = pit(x, layer_data(p_dens, 2L), 1), interpolate_adj = T)\nfor (idx in 1:n_rep) {\n  gg_data = data.frame(\n    PIT = seq(0,1,length.out = N),\n    ECDF = ecdf(pit(x_post[rep_id == idx], layer_data(p_dens), idx))(seq(0,1,length.out = N))\n    )\n  p_ecdf <- p_ecdf + geom_step(data = gg_data, aes(x = PIT, y = ECDF), colour = layer_data(p_dens)$colour[1], alpha = .7, linewidth = layer_data(p_dens)$linewidth[1])\n}\n\n\n\n\nCode\np_ecdfd <- ppc_pit_ecdf(pit = pit(x, layer_data(p_dens, 2L), 1), interpolate_adj = T, plot_diff = T)\nfor (idx in 1:n_rep) {\n  gg_data = data.frame(\n    PIT = seq(0,1,length.out = N),\n    ECDF = ecdf(pit(x_post[rep_id == idx], layer_data(p_dens), idx))(seq(0,1,length.out = N))\n    )\n  p_ecdfd <- p_ecdfd + geom_step(data = gg_data, aes(x = PIT, y = ECDF - PIT), colour = layer_data(p_dens)$colour[1], alpha = .7, linewidth = layer_data(p_dens)$linewidth[1])\n}\n\n\n\n\nCode\np_dens\n\n\n\n\n\nCode\np_ecdf\n\n\n\n\n\nCode\np_ecdfd\n\n\n\n\n\nCode\np_dots\n\n\n\n\n\nCode\np_hist\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "case_studies/count/roaches.html",
    "href": "case_studies/count/roaches.html",
    "title": "Rootograms for count data",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(cmdstanr)\nlibrary(brms)\n\nsource(\"../../code/helpers.R\")\n\n\ntext"
  },
  {
    "objectID": "case_studies/continuous/kde_study/KDE_demo.html",
    "href": "case_studies/continuous/kde_study/KDE_demo.html",
    "title": "Pitfalls of density plots",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(bayesplot)\nlibrary(khroma)\nlibrary(latex2exp)\nlibrary(sfsmisc)\nsource(\"../../../code/helpers.R\")\nsource(\"../../../code/kde_tests.R\")\n\nset.seed(37645624)\n\ngood_theme &lt;- bayesplot::theme_default(base_family = \"Sans\", base_size = 14) + theme(\n  axis.text = element_text(colour = \"#666666\", size = 12),\n  axis.ticks = element_line(colour = \"#666666\"),\n  title = element_text(colour = \"#666666\", size = 16),\n  plot.subtitle = element_text(colour = \"#666666\", size = 14),\n  legend.text = element_text(colour = \"#666666\", size = 12),\n  legend.title = element_text(colour = \"#666666\", size = 14),\n  axis.line = element_line(colour = \"#666666\"))\n\ntheme_set(good_theme)\nCode\necdf_difference_limits &lt;- ecdf_confidence_intervals(\n  gamma = adjust_gamma_optimize(1000,1000, .95),\n  N = 1000, K = 1000)\n\nx0 &lt;- 0:1000 / 1000\n\np0 &lt;- ggplot(mapping = aes(x = x0)) +\n  geom_step(aes(y = (ecdf_difference_limits$lower - 0:1000) / 1000)) +\n  geom_step(aes(y = (ecdf_difference_limits$upper - 0:1000) / 1000)) +\n  labs(title = \"ECDF Difference\",\n       subtitle = \"95% outer simultaneous confidense intervals\",\n       x = \"PIT\",\n       y = \"ECDF - Difference\",\n       colour = \"Method\")"
  },
  {
    "objectID": "case_studies/continuous/kde_study/KDE_demo.html#intro",
    "href": "case_studies/continuous/kde_study/KDE_demo.html#intro",
    "title": "Pitfalls of density plots",
    "section": "Intro",
    "text": "Intro\nBelow, we examine two common pitfalls of density plots made using kernel density estimates (KDEs). Overall, KDEs have satisfactory performance in summarising observations of continuous random variables, but, as highlighted below, when the variable is bounded, or contains point masses, the density estimate can misrepresent the data without any issues being immediately apparent from the density plot.\nIncluded is also a comparison between the density estimates provided by ggplot and ggdist. By default, ggdist attempts to automatically detect he bounds of the observed data."
  },
  {
    "objectID": "case_studies/continuous/kde_study/KDE_demo.html#simple-continuous-example",
    "href": "case_studies/continuous/kde_study/KDE_demo.html#simple-continuous-example",
    "title": "Pitfalls of density plots",
    "section": "Simple continuous example",
    "text": "Simple continuous example\nSample \\(x_1,\\dots,x_N\\), from Laplace distribution with mean, \\(\\mu = 0\\), and scale, \\(b=1\\), that is, the density function of \\(x\\) is \\(f(x) = \\frac{1}{2}\\exp\\left(-|x|\\right)\\).\n\n\nCode\nx1 &lt;- sample(c(-1,1), 1000, replace = T) * rexp(1000)\n\n\n\n\nCode\np1 &lt;- ggplot(mapping = aes(x = x1)) +\n  stat_density(aes(colour = \"ggplot\"), bw = \"SJ\", geom = \"line\") +\n  stat_slab(aes(colour = \"ggdist\"),  fill = NA, normalize = \"none\", scale = 1) +\n  labs(\"Laplace distribution\",subtitle = \"N = 1000\\nx ~ 1/2 exp(-|x|)\",\n       x = TeX(\"$x$\"), y = \"KDE\", colour = \"Method\") +\n  scale_y_continuous(expand = c(0,NA)) +\n  coord_cartesian(ylim = c(0,1.1 * max(density(x1)$y)))\n\np1\n\n\n\n\n\nThe ECDF difference plot doesn’t suggest any issues with the KDE\n\n\nCode\np0 + geom_step(aes(y = ecdf(pit_from_densityplot(p1, 1, x1))(x0) - x0,\n                   colour = \"ggplot\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p1, 2, x1, T))(x0) - x0,\n                   colour = \"ggdist\")) +\n  scale_colour_vibrant(limits = c(\"ggdist\", \"ggplot\", \"1\", \"2\"), \n                       breaks = c(\"ggdist\", \"ggplot\"))"
  },
  {
    "objectID": "case_studies/continuous/kde_study/KDE_demo.html#bounded-continuous-data",
    "href": "case_studies/continuous/kde_study/KDE_demo.html#bounded-continuous-data",
    "title": "Pitfalls of density plots",
    "section": "Bounded continuous data",
    "text": "Bounded continuous data\nSample as above, but the distribution is truncated to the interval \\([-1.5,0.5]\\).\n\n\nCode\nx2 &lt;- x1\nwhile (!all(x2 &gt; -1.5, x2 &lt; .5)) {\n  mask &lt;- (x2 &lt; -1.5 | x2 &gt; .5)\n  x2[mask] = sample(c(-1,1), sum(mask), replace = T) * rexp(sum(mask))\n}\n\n\nWe see a difference in the tails of the densityplots, when the boundedness is taken into account in the KDE computations.\n\n\nCode\np2 &lt;- ggplot(mapping = aes(x = x2)) + \n  stat_density(aes(colour = \"Bounded\"), bw = \"SJ\", geom = \"line\", bounds = c(-1.5, .5)) +\n  stat_density(aes(colour = \"Naive\"), bw = \"SJ\", geom = \"line\") +\n  stat_slab(aes(colour = \"ggdist\"), fill = NA, normalize = \"none\", scale = 1) +\n  labs(title = \"Bounded observation\", subtitle = \"KDE computed with and without the bounds\",\n       x = TeX(\"$x$\"), y = \"KDE\", colour = \"Method\") +\n  scale_colour_vibrant(limits = c(\"ggdist\", \"Naive\", \"Bounded\", \"2\"), \n                       breaks = c(\"ggdist\", \"Naive\", \"Bounded\")) +\n  scale_y_continuous(expand = c(0,NA))\n\np2\n\n\n\n\n\nThe ECDF difference plot also suggests calibration issues with the unbounded KDE. Both the bounded ggplot density and the automatic bound detection by ggdist look well calibrated.\n\n\nCode\np0 + geom_step(aes(y = ecdf(pit_from_densityplot(p2, 2, x2))(x0) - x0,\n                   colour = \"Naive\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p2, 1, x2))(x0) - x0, colour = \"Bounded\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p2, 3, x2, T))(x0) - x0,\n                   colour = \"ggdist\")) +\n  labs(colour = \"Method\") +\n  scale_colour_vibrant(limits = c(\"ggdist\", \"Naive\", \"Bounded\", \"2\"), \n                       breaks = c(\"ggdist\", \"Naive\", \"Bounded\"))"
  },
  {
    "objectID": "case_studies/continuous/kde_study/KDE_demo.html#continuous-data-with-point-masses",
    "href": "case_studies/continuous/kde_study/KDE_demo.html#continuous-data-with-point-masses",
    "title": "Pitfalls of density plots",
    "section": "Continuous data with point masses",
    "text": "Continuous data with point masses\nSimulate an example, where measurements under a certain threshold are reported as zeros. Here \\(x\\sim\\exp(x)\\) with all values sampled below \\(0.1\\) reported as zero.\n\n\nCode\nx3 &lt;- rexp(1000, 1)\nx3[x3 &lt; .1] &lt;- 0\n\n\nIf the measurement threshold is known, the data can be visualized as a mixture of a discrete point mass and a continuous distribution. The “naive” KDE seems to under estimate the density near \\(0.1\\), and can’t show the spike at zero.\nBelow, the density of the continuous mixture component is scaled by \\(P(x \\neq 0)\\) to make the area of the bar and the KDE together add to one.\n\n\nCode\np3 &lt;- ggplot() + \n  geom_rect(\n    aes(xmin = 0, xmax = .1, ymin = 0, ymax = 10 * sum(x3 == 0) / length(x3), \n        colour = \"Mixture\", fill = \"Mixture\"),\n    alpha = .3) +\n  stat_density(\n    aes(x = x3[x3 &gt; 0], y = (1 - sum(x3 == 0) / length(x3)) * after_stat(density), \n        colour = \"Mixture\", fill = \"Naive\"), bw = \"SJ\", alpha = 0, bounds = c(.1, Inf), outline.type = \"upper\") +\n  stat_density(aes(x = x3, colour = \"Naive\", fill = \"Naive\"), bw = \"SJ\", alpha = 0, linewidth = 1, outline.type = \"upper\") +\n  stat_slab(aes(x = x3, colour = \"ggdist - full\", fill = \"ggdist - full\"),\n             slab_fill = NA, normalize = \"none\", scale = 1) +\n  labs(title = \"Observation with point mass\",\n       subtitle = \"KDE for full observation and only continuous part\",\n       x = TeX(\"x\"), y = \"\", colour = \"Method\", fill = \"Method\") +\n  scale_colour_vibrant(limits = c(\"ggdist - full\", \"Naive\", \"Mixture\", \"2\"), \n                       breaks = c(\"ggdist - full\", \"Naive\", \"Mixture\")) +\n  scale_fill_vibrant(limits = c(\"ggdist - full\", \"Naive\", \"Mixture\", \"2\"),\n                     breaks = c(\"ggdist - full\", \"Naive\", \"Mixture\")) +\n  scale_y_continuous(expand = c(0,NA))\n\np3\n\n\n\n\n\nThe calibration of the KDE for the continuous part should be inspected separately, as the scaled density above would show miscalibration due to missing part of the observation distribution.\n\n\nCode\np4 &lt;- ggplot() + \n  stat_density(\n    aes(x = x3[x3 &gt; 0], \n        colour = \"Mixture\"), bw = \"SJ\", bounds = c(.1, Inf),\n    geom = \"line\") +\n  stat_slab(aes(x = x3[x3 &gt; 0], colour = \"ggdist - cont.\"),\n            slab_fill = NA, normalize = \"none\", scale = 1, fill = NA) +\n  labs(colour = \"Method\") +\n  scale_colour_vibrant(limits = c(\"ggdist\", \"Naive\", \"Mixture - cont.\", \"ggdist - cont.\"), \n                       breaks = c(\"Mixture\", \"ggdist - cont.\"))\n\np4\n\n\nWarning: Removed 512 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAgain, the ECDF difference plot suggest issues with the naive approach and thus could be used to warn user if the naive density plot is drawn.\nHere the automatic bound detection by ggdist produces a KDE that is well calibrated after the over saturation of PIT values of zero and the sudden drop where no observations are covered by the density estimate. When restricted to the continuous part of the mixture, both the ggplot and ggdist methods give calibrated estimates.\n\n\nCode\np0 + geom_step(aes(y = ecdf(pit_from_densityplot(p3, 3, x3))(x0) - x0, colour = \"Naive\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p4, 1, x3[x3 &gt; 0]))(x0) - x0, \n                colour = \"Mixture\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p3, 4, x3, T))(x0) - x0,\n                   colour = \"ggdist - full\")) +\n  geom_step(aes(y = ecdf(pit_from_densityplot(p4, 2, x3[x3 &gt; 0], T))(x0) - x0,\n                   colour = \"ggdist - cont.\")) +\n  labs(colour = \"Method\") +\n  scale_colour_vibrant(limits = c(\"ggdist - full\", \"Naive\", \"Mixture - cont.\", \"ggdist - cont.\"), \n                       breaks = c(\"ggdist - full\", \"Naive\", \"Mixture - cont.\", \"ggdist - cont.\"))\n\n\nWarning: Removed 1001 rows containing missing values (`geom_step()`)."
  }
]